# ğŸ§  Deep Qâ€‘Network (DQN) on CartPole

An implementation of a Deep Qâ€‘Network (DQN) in PyTorch, trained to solve the classic **CartPoleâ€‘v1** environment from OpenAI Gym. The agent learns to balance the pole by approximating Qâ€‘values with a neural network, using experience replay and a target network for stability.

---

## ğŸ¥ Results

Hereâ€™s the trained agent balancing the pole:

[â–¶ï¸ Watch full video](results/videos/cartpole_dqn.mp4)

---

## ğŸ“ˆ Training Progress

![Training Curve](results/plots/Reward_curve_cartpole.png)

- The agent starts with random play (low rewards).  
- Over time, it learns to balance longer, with rewards approaching the maximum of 200.  
- Occasional spikes/drops reflect exploration during training.

---

## âš™ï¸ How It Works

- **Replay Buffer**: Stores past experiences `(state, action, reward, next_state, done)` and samples random minibatches for training.  
- **Bellman Update**: Targets are computed as  
  y = r + Î³ * max_a' Q_target(s', a')  
- **Target Network**: A copy of the Qâ€‘network updated periodically to stabilize training.  
- **Epsilonâ€‘Greedy Exploration**: Balances exploration vs. exploitation by decaying Îµ over time.

---

## ğŸš€ Getting Started

### Installation
```

bash
git clone https://github.com/yourusername/dqn-cartpole.git
cd dqn-cartpole
pip install -r requirements.txt
```

### Training
```bash
python train.py
```
### Watching the Agent
# If you want to see the trained agent play, run the following inside your notebook
# or modify train.py to load a saved model and call env.render():
# Example (inside Python):
#   model.load_state_dict(torch.load("checkpoints/dqn_cartpole.pth"))
#   state, _ = env.reset()
#   for _ in range(200):
#       action = model(torch.tensor(state, dtype=torch.float32).unsqueeze(0)).argmax().item()
#       state, reward, done, _, _ = env.step(action)
#       env.render()
#       if done: break
---

## ğŸ“š What I Learned

- How to implement a replay buffer and use it for stable training.  
- Why target networks are essential to prevent divergence.  
- How the Bellman equation translates into a neural network update.  
- Debugging PyTorch models and handling Gymâ€™s evolving API.  

---

## ğŸ”® Next Steps

- Extend to **Double DQN** to reduce overestimation bias.  
- Try **LunarLanderâ€‘v2** for a harder control task.  
- Add **Prioritized Experience Replay** for more efficient learning.  

---

## ğŸ™Œ Acknowledgments

- OpenAI Gym for the CartPole environment.  
- PyTorch for the deep learning framework.  
- DeepMindâ€™s original DQN paper for inspiration.  
